from typing import List
from llama_index.core import Document, PropertyGraphIndex, Settings
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor
from llama_index.core.schema import NodeWithScore
from llama_index.llms.anthropic import Anthropic

class JITGraphAnalyzer:
    def __init__(self, model_name: str = "claude-sonnet-4-0"):
        self.llm = Anthropic(model=model_name, timeout=300.0)
        Settings.llm = self.llm

    def build_and_analyze(self, retrieved_nodes: List[NodeWithScore], query_str: str, use_hexagon_report: bool = True):
        """
        ê²€ìƒ‰ëœ ë…¸ë“œë“¤ë¡œë¶€í„° ì¦‰ì„ì—ì„œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê³  ìœ¡ê°í˜• ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ—ï¸ ì„ ë³„ëœ {len(retrieved_nodes)}ê°œ ë¬¸ì„œë¡œë¶€í„° ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘...")
        
        # 1. ë¬¸ì„œí™”
        documents = [Document(text=n.node.get_content(), metadata=n.node.metadata) for n in retrieved_nodes]
        
        # 2. ì¶”ì¶œê¸° ì„¤ì •
        extractor = SimpleLLMPathExtractor(
            llm=self.llm,
            num_workers=2
        )
        
        # 3. JIT ì§€ì‹ ê·¸ë˜í”„ ìƒì„± (ë©”ëª¨ë¦¬ ìƒì— êµ¬ì¶•)
        index = PropertyGraphIndex.from_documents(
            documents,
            kg_extractors=[extractor],
            show_progress=True
        )
        
        print("ğŸ” ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„ ë° ìœ¡ê°í˜• ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        # 4. ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        query_engine = index.as_query_engine(include_text=True)
        
        if use_hexagon_report:
            from llama_index.core import PromptTemplate
            hexagon_prompt = (
                "ë‹¹ì‹ ì€ 'ê·¸ë˜í”„ ì§€ëŠ¥ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤. ì œê³µëœ [ì§€ì‹ ê·¸ë˜í”„ ì •ë³´]ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ê³ , "
                "ì•„ë˜ì˜ **ìœ¡ê°í˜• ê°ê´€ì  ê³„ì‚° ë¡œì§**ì— ë”°ë¼ ë¶„ì„ í’ˆì§ˆì„ ì ìˆ˜í™”í•˜ì„¸ìš”.\n\n"
                "### [ìœ¡ê°í˜• ì ìˆ˜ ê³„ì‚° ë¡œì§]\n"
                "1. ì‚¬ì‹¤ì„± (Factuality) = (ë§¤ì¹­ ì—”í‹°í‹° ìˆ˜ / ë‹µë³€ ë‚´ ì „ì²´ ì—”í‹°í‹° ìˆ˜) * 100\n"
                "2. ë…ì°½ì„± (Originality) = 30 + (Max ê²½ë¡œ ê¹Šì´ * 20) + (ê³ ìœ  ê´€ê³„ ìˆ˜ * 15) [ìµœëŒ€ 100]\n"
                "3. ì—°ê²°ì„± (Connectivity) = (ì—°ê²°ëœ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë“œ ë„ë©”ì¸ ìˆ˜ / 3) * 100 [ìµœëŒ€ 100]\n"
                "4. ì •ë³´ ë°€ë„ (Density) = (ì‚¬ìš©ëœ Triplet ìˆ˜ / ë¬¸ì¥ ìˆ˜) * 33 [ìµœëŒ€ 100]\n"
                "5. ì£¼ì œ ì§‘ì¤‘ë„ (Relevance) = (í•µì‹¬ ì£¼ì œ ê´€ë ¨ Triplet ìˆ˜ / ì „ì²´ ì‚¬ìš© Triplet ìˆ˜) * 100\n"
                "6. ë…¼ë¦¬ ì •í•©ì„± (Consistency) = 100 - (ìƒì¶© ê±´ìˆ˜ * 25) [ìµœì†Œ 0]\n\n"
                "ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ì„ **ë°˜ë“œì‹œ** ì§€ì»¤ ë‹µë³€í•˜ì„¸ìš”:\n"
                "---ë‹µë³€---\n"
                "[ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ í•µì‹¬ ë¶„ì„ ë‚´ìš©]\n\n"
                "---ìœ¡ê°í˜• ë¶„ì„ ë¦¬í¬íŠ¸---\n"
                "- ì‚¬ì‹¤ì„±: [ê²°ê³¼]/100\n- ë…ì°½ì„±: [ê²°ê³¼]/100\n- ì—°ê²°ì„±: [ê²°ê³¼]/100\n"
                "- ì •ë³´ ë°€ë„: [ê²°ê³¼]/100\n- ì£¼ì œ ì§‘ì¤‘ë„: [ê²°ê³¼]/100\n- ë…¼ë¦¬ ì •í•©ì„±: [ê²°ê³¼]/100\n"
                "- ì¢…í•© í‰ì : [ìœ„ 6ê°œ ì ìˆ˜ì˜ í‰ê· ]\n\n"
                "ì§ˆë¬¸: {query_str}\n"
                "ì§€ì‹ ê·¸ë˜í”„ ì •ë³´: {context_str}\n\n"
                "ë‹µë³€:"
            )
            query_engine.update_prompts({
                "response_synthesizer:text_qa_template": PromptTemplate(hexagon_prompt)
            })
            
        response = query_engine.query(query_str)
        return response, index

    def get_graph_triples(self, index: PropertyGraphIndex):
        """
        êµ¬ì¶•ëœ ê·¸ë˜í”„ì—ì„œ ì¶”ì¶œëœ íŠ¸ë¦¬í”Œë“¤ì„ ì‹œê°í™”ìš© ë°ì´í„° ë“±ìœ¼ë¡œ ì¶”ì¶œ
        """
        # í˜„ì¬ LlamaIndex APIë¥¼ í†µí•´ ì¶”ì¶œëœ ëª¨ë“  íŠ¸ë¦¬í”Œ ë°˜í™˜
        return index.property_graph_store.get_triplets()
