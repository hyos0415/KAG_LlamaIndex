import os
from typing import List, Optional, Tuple
from llama_index.core import Document, PropertyGraphIndex, Settings, StorageContext
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor
from llama_index.core.schema import NodeWithScore
from llama_index.llms.anthropic import Anthropic
from app.etl.storage import StorageManager
import re

class KnowledgeGraphManager:
    def __init__(self, model_name: str = "claude-sonnet-4-0"):
        self.llm = Anthropic(model=model_name, timeout=300.0)
        Settings.llm = self.llm
        self.storage_manager = StorageManager()

    def sync_to_neo4j(self, retrieved_nodes: List[NodeWithScore], label: str = "Article"):
        """
        ê²€ìƒ‰ëœ ë…¸ë“œë“¤ë¡œë¶€í„° ì§€ì‹ì„ ì¶”ì¶œí•˜ì—¬ Neo4jì— ì €ì¥í•©ë‹ˆë‹¤.
        ë¼ë²¨ì„ í†µí•´ 'ê²€ì¦ëœ ê¸°ì‚¬(Article)'ì™€ 'ì‚¬ìš©ì ì´ˆì•ˆ(Draft)'ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ”— {len(retrieved_nodes)}ê°œ ë¬¸ì„œì˜ ì§€ì‹ì„ Neo4j({label})ë¡œ ë™ê¸°í™” ì¤‘...")
        documents = [Document(text=n.node.get_content(), metadata={**n.node.metadata, "type": label}) for n in retrieved_nodes]
        
        # Neo4j ìŠ¤í† ë¦¬ì§€ ì¤€ë¹„
        graph_store = self.storage_manager.get_neo4j_graph_store()
        
        # ì¶”ì¶œê¸° ì„¤ì • (ë…¸ë“œì— ë¼ë²¨ ë¶€ì—¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°€ì´ë“œ í¬í•¨ ê°€ëŠ¥ - ì—¬ê¸°ì„  ë©”íƒ€ë°ì´í„° í™œìš©)
        extractor = SimpleLLMPathExtractor(llm=self.llm, num_workers=2)
        
        # Neo4jì— ì €ì¥
        index = PropertyGraphIndex.from_documents(
            documents,
            property_graph_store=graph_store,
            kg_extractors=[extractor],
            show_progress=True
        )
        return index

    async def calculate_hexagonal_metrics(self, query_str: str, results: list) -> dict:
        """
        Neo4j ë°ì´í„°ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ 6ê°€ì§€ ìœ¡ê°í˜• ì§€í‘œë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
        """
        print("ğŸ“Š [Metric Calculation] ìœ¡ê°í˜• ì§€í‘œ ì‚°ì¶œ ì¤‘...")
        graph_store = self.storage_manager.get_neo4j_graph_store()
        
        metrics = {
            "connectivity": 0,
            "factuality": 0,
            "depth": 0,
            "originality": 0,
            "density": 0,
            "insight": 0
        }

        try:
            # 1. ì—°ê²°ì„± (Connectivity): ê²°ê³¼ ë…¸ë“œë“¤ì˜ í‰ê·  ì°¨ìˆ˜(Degree)
            # ê²°ê³¼ ë…¸ë“œ ID ì¶”ì¶œ (ê²°ê³¼ê°€ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •í•˜ê±°ë‚˜ ì¿¼ë¦¬ ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
            connectivity_query = "MATCH (n)-[r]-() RETURN count(r) as connections, count(distinct n) as nodes"
            conn_res = graph_store.query(connectivity_query)
            if conn_res and conn_res[0]['nodes'] > 0:
                # ì „ì²´ ê·¸ë˜í”„ì˜ í‰ê·  ì—°ê²° ë°€ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™” (0~100)
                avg_conn = conn_res[0]['connections'] / conn_res[0]['nodes']
                metrics["connectivity"] = min(100, int(avg_conn * 10))

            # 2. ì‚¬ì‹¤ì„± (Factuality): LLMì´ ê²°ê³¼ì˜ ê·¼ê±°ê°€ ì–¼ë§ˆë‚˜ ëª…í™•í•œì§€ í‰ê°€
            fact_prompt = (
                "ì œê³µëœ [ë°ì´í„°]ê°€ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ìƒí˜¸ ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤(ì—”í‹°í‹° ê°„ ê´€ê³„)ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
                "ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                f"ë°ì´í„°: {results}"
            )
            metrics["factuality"] = int(str(self.llm.complete(fact_prompt)).strip() or 0)

            # 3. ì‹¬ì¸µì„± (Depth): 2-hop ì´ìƒì˜ ê´€ê³„ ì¡´ì¬ ì—¬ë¶€
            depth_query = "MATCH path=(n)-[*2..3]-(m) RETURN count(path) as paths LIMIT 1"
            depth_res = graph_store.query(depth_query)
            metrics["depth"] = min(100, int(depth_res[0]['paths'] * 5)) if depth_res else 0

            # 4. ë…ì°½ì„± (Originality): ê¸°ì¡´ ì§€ì‹ ëŒ€ë¹„ ì‹ ê·œ ì •ë³´ì˜ ê°€ì¹˜ í‰ê°€ (LLM)
            originality_prompt = (
                "ì œê³µëœ [ê²°ê³¼]ê°€ ì¼ë°˜ì ì¸ ìƒì‹ì´ë‚˜ ê¸°ì¡´ ë³´ë„ ë‚´ìš©ì„ ë„˜ì–´ ì–¼ë§ˆë‚˜ ê³ ìœ í•˜ê³  êµ¬ì²´ì ì¸ ìƒˆë¡œìš´ ì •ë³´(ì‹ ê·œ ì¸ë¬¼, íŠ¹ì • ìˆ˜ì¹˜, ë¯¸ì‹œì  ì‚¬ê±´ ë“±)ë¥¼ ë‹´ê³  ìˆëŠ”ì§€ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
                "ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                f"ê²°ê³¼: {results}"
            )
            metrics["originality"] = int(str(self.llm.complete(originality_prompt)).strip() or 0)

            # 5. ì •ë³´ ë°€ë„ (Density): ê²°ê³¼ ë‚´ ê´€ê³„ ìˆ˜ / ë…¸ë“œ ìˆ˜
            if results and isinstance(results, list):
                rel_count = len(results)
                node_ids = set()
                for r in results:
                    if isinstance(r, dict):
                        node_ids.update(r.values())
                metrics["density"] = min(100, int((rel_count / max(1, len(node_ids))) * 20))

            # 6. ì£¼ì œ í†µì°° (Insight): ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í‰ê°€ (LLM)
            insight_prompt = (
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ í•µì‹¬ì ì¸ í†µì°°ì„ ì£¼ëŠ”ì§€ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
                "ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                f"ì§ˆë¬¸: {query_str}\n"
                f"ê²°ê³¼: {results}"
            )
            metrics["insight"] = int(str(self.llm.complete(insight_prompt)).strip() or 0)

        except Exception as e:
            print(f"âš ï¸ ì§€í‘œ ì‚°ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return metrics

    async def analyze_with_cypher(self, query_str: str):
        """
        [Direct Cypher Engine + Hexagonal Analysis 2.0]
        """
        print(f"ğŸ§  [Direct Analysis] Cypher ì¶”ë¡  ì¤‘: {query_str}")
        
        graph_store = self.storage_manager.get_neo4j_graph_store()
        schema_str = await graph_store.aget_schema_str()
        
        prompt = (
            "ë‹¹ì‹ ì€ Neo4j Cypher ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ [ìŠ¤í‚¤ë§ˆ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì‚¬ìš©ì ì§ˆë¬¸]ì— ë‹µí•  ìˆ˜ ìˆëŠ” Cypher ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
            "**ì£¼ì˜**: í˜„ì¬ ë°ì´í„°ì…‹ì´ ì‘ìœ¼ë¯€ë¡œ `count(*) > 1`ê³¼ ê°™ì€ ì—„ê²©í•œ í•„í„°ë§ì€ í”¼í•˜ê³ , ìµœëŒ€í•œ ë§ì€ ê´€ê³„ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆë„ë¡ ì‘ì„±í•˜ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ **Cypher ì¿¼ë¦¬ë¬¸ë§Œ** ì¶œë ¥í•˜ê³ , ë¶€ì—° ì„¤ëª…ì´ë‚˜ ì½”ë“œ ë¸”ë¡(```)ì€ ìƒëµí•˜ì„¸ìš”.\n\n"
            f"### [ìŠ¤í‚¤ë§ˆ ì •ë³´]\n{schema_str}\n\n"
            f"### [ì‚¬ìš©ì ì§ˆë¬¸]\n{query_str}\n\n"
            "Cypher ì¿¼ë¦¬:"
        )
        
        llm_response = self.llm.complete(prompt)
        cypher_query = str(llm_response).strip().replace("```cypher", "").replace("```", "")
        print(f"ğŸ“Ÿ ìƒì„±ëœ Cypher: {cypher_query}")
        
        try:
            results = graph_store.query(cypher_query)
            
            # [í•µì‹¬] ìœ¡ê°í˜• ì§€í‘œ ì‚°ì¶œ (ê²°ê³¼ê°€ ì—†ë”ë¼ë„ ê·¸ë˜í”„ ì „ì²´ í†µê³„ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•˜ë„ë¡ ë¡œì§ ë‚´ì—ì„œ ì²˜ë¦¬)
            metrics = await self.calculate_hexagonal_metrics(query_str, results or [])

            if not results:
                answer = "ğŸ” íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ëŠ” ì—†ìœ¼ë‚˜, ì „ì²´ ì§€ì‹ ê·¸ë˜í”„ì˜ í†µê³„ì  ìˆ˜ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•´ ë“œë¦½ë‹ˆë‹¤."
            else:
                # ê²°ê³¼ í•´ì„ ìš”ì²­ (LLM)
                interpret_prompt = (
                    "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. [ì¡°íšŒ ê²°ê³¼]ì™€ [ìœ¡ê°í˜• ë¶„ì„ ìˆ˜ì¹˜]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "ì¸ë¬¼/ê¸°ì—… ê°„ì˜ ê´€ê³„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ë§ˆì§€ë§‰ì—ëŠ” ë¶„ì„ ìˆ˜ì¹˜ì— ëŒ€í•œ ê·¼ê±°ë¥¼ ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
                    f"### [ì‚¬ìš©ì ì§ˆë¬¸]\n{query_str}\n"
                    f"### [ì¡°íšŒ ê²°ê³¼]\n{results}\n"
                    f"### [ìœ¡ê°í˜• ë¶„ì„ ìˆ˜ì¹˜]\n{metrics}\n\n"
                    "ë¶„ì„ ë‹µë³€:"
                )
                answer = self.llm.complete(interpret_prompt)

            return {
                "answer": answer,
                "metrics": metrics,
                "cypher": cypher_query
            }
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


    async def validate_user_article(self, user_article_text: str, context_nodes: List[NodeWithScore]):
        """
        ì‚¬ìš©ì ê¸°ì‚¬ì™€ ê²€ìƒ‰ëœ ìœ ì‚¬ ê¸°ì‚¬ë“¤(Context)ì„ ëŒ€ì¡°í•˜ì—¬ 
        ì‚¬ì‹¤ì„±(Factuality)ê³¼ ë…ì°½ì„±(Originality)ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        print("ğŸ” [Validation] ì‚¬ìš©ì ê¸°ì‚¬ vs ê²€ìƒ‰ ì§€ì‹ ëŒ€ì¡° ë¶„ì„ ì‹œì‘...")
        
        # 1. ê¸°ì¡´ ë°ì´í„° ì •ë¦¬ (ê¹¨ë—í•œ ëŒ€ì¡°ë¥¼ ìœ„í•´)
        graph_store = self.storage_manager.get_neo4j_graph_store()
        graph_store.query("MATCH (n) DETACH DELETE n")
        
        # 2. Context ê¸°ì‚¬ë“¤ì˜ ì§€ì‹ì„ Neo4jì— ë¡œë“œ (VerifiedSource ë¼ë²¨ ë¶€ì—¬)
        self.sync_to_neo4j(context_nodes, label="VerifiedSource")
        
        # [í•µì‹¬] ë¡œë“œëœ ì§€ì‹ì„ ëª…ì‹œì ìœ¼ë¡œ í™•ì¸
        existing_knowledge = graph_store.query(
            "MATCH (n)-[r]->(m) RETURN n.id as source, type(r) as relation, m.id as target LIMIT 100"
        )
        
        # 3. ì‚¬ìš©ì ê¸°ì‚¬ë¡œë¶€í„° íŠ¸ë¦¬í”Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ìƒì—ì„œë§Œ ìˆ˜í–‰í•˜ì—¬ ê²©ë¦¬ ìœ ì§€)
        extractor = SimpleLLMPathExtractor(llm=self.llm)
        temp_doc = Document(text=user_article_text)
        user_triplets = await extractor.acall([temp_doc])
        
        # 4. ì‚¬ì‹¤ì„± ë° ë…ì°½ì„± êµì°¨ ê²€ì¦ (LLM)
        fact_check_prompt = (
            "ë‹¹ì‹ ì€ ë‰´ìŠ¤ íŒ©íŠ¸ì²´í¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. [ì‚¬ìš©ì ì£¼ì¥]ë“¤ì´ [ê²€ì¦ëœ ê¸°ì¡´ ì§€ì‹]ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€, "
            "ëª¨ìˆœë˜ëŠ”ì§€, ì•„ë‹ˆë©´ ìƒˆë¡œìš´ ì •ë³´ì¸ì§€ íŒë³„í•˜ì„¸ìš”.\n\n"
            f"### [ê²€ì¦ëœ ê¸°ì¡´ ì§€ì‹]\n{existing_knowledge}\n\n"
            f"### [ì‚¬ìš©ì ì£¼ì¥]\n{user_triplets}\n\n"
            "ì¼ì¹˜í•˜ë©´ 'Factual', ëª¨ìˆœë˜ë©´ 'Contradiction', ì—†ìœ¼ë©´ 'New Information'ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  "
            "ê·¸ ê·¼ê±°ë¥¼ ê´€ê³„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
            "íŒë³„ ê²°ê³¼:"
        )
        validation_res = self.llm.complete(fact_check_prompt)
        
        # 5. ìœ¡ê°í˜• ì§€í‘œ ì‚°ì¶œ
        metrics = await self.calculate_hexagonal_metrics(user_article_text, user_triplets)
        
        return {
            "validation_report": validation_res,
            "existing_knowledge": existing_knowledge,
            "user_triplets": user_triplets,
            "metrics": metrics
        }
