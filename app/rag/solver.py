import asyncio
from typing import List, Optional
from llama_index.core import Settings, StorageContext, VectorStoreIndex, QueryBundle
from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.openai import OpenAIEmbedding

from app.etl.storage import StorageManager
from app.graph.jit_builder import JITGraphAnalyzer
from app.rag.decomposer import QueryDecomposer

class HybridRetriever(BaseRetriever):
    def __init__(self, vector_retriever: VectorIndexRetriever, bm25_retriever: BM25Retriever):
        self._vector_retriever = vector_retriever
        self._bm25_retriever = bm25_retriever
        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle):
        vector_nodes = self._vector_retriever.retrieve(query_bundle)
        bm25_nodes = self._bm25_retriever.retrieve(query_bundle)

        # ë‹¨ìˆœ í•©ì§‘í•© ë° ì¤‘ë³µ ì œê±°
        all_nodes_dict = {n.node.node_id: n for n in vector_nodes}
        for n in bm25_nodes:
            if n.node.node_id not in all_nodes_dict:
                all_nodes_dict[n.node.node_id] = n
        
        return list(all_nodes_dict.values())

class NewsRAGSolver:
    def __init__(self, model_name: str = "claude-sonnet-4-0", db_url: Optional[str] = None, chroma_path: Optional[str] = None):
        self.llm = Anthropic(model=model_name, timeout=300.0)
        self.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        
        self.storage_manager = StorageManager(db_url=db_url, chroma_path=chroma_path)
        self.graph_analyzer = JITGraphAnalyzer(model_name=model_name)
        self.decomposer = QueryDecomposer(model_name=model_name)
        
    async def query(self, query_str: str, top_k: int = 4, use_graph: bool = True):
        """
        Elasticsearch ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í›„ ì§€ì‹ ê·¸ë˜í”„ ë”¥ë‹¤ì´ë¸Œ ë¶„ì„ ìˆ˜í–‰
        """
        # 1. ì¸ë±ìŠ¤ ë¡œë“œ (Elasticsearch)
        vector_index = VectorStoreIndex.from_vector_store(
            self.storage_manager.es_store
        )
        
        # 2. Elasticsearch í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        retriever = vector_index.as_retriever(
            similarity_top_k=top_k,
            vector_store_query_mode="hybrid",
            alpha=0.5
        )
        
        # 3. ê²°ê³¼ ì¶”ì¶œ ë° ë¦¬ë­í‚¹
        retrieved_nodes = retriever.retrieve(query_str)
        reranker = LLMRerank(choice_batch_size=5, top_n=top_k // 2 if top_k > 2 else top_k, llm=self.llm)
        final_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle=QueryBundle(query_str))
        
        if use_graph and final_nodes:
            # 4. JIT ì§€ì‹ ê·¸ë˜í”„ ë¶„ì„
            print(f"ğŸ§  {len(final_nodes)}ê°œ í•µì‹¬ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            response, _ = self.graph_analyzer.build_and_analyze(final_nodes, query_str)
            return response
        else:
            query_engine = RetrieverQueryEngine.from_args(
                retriever=retriever,
                node_postprocessors=[reranker]
            )
            return query_engine.query(query_str)

    async def recommend_similar_articles(self, article_content: str, top_k: int = 5):
        """
        ì…ë ¥ëœ ê¸°ì‚¬ ì›ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ì°¨ì› ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ìœ ì‚¬í•œ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
        """
        # 1. ê¸°ì‚¬ ë¶„ì„ ë° í˜ì´ì…‹ ë„ì¶œ
        analysis_result = await self.decomposer.decompose_article(article_content)
        
        # 2. ê° í˜ì´ì…‹ë³„ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
        search_tasks = []
        for facet in analysis_result.facets:
            search_tasks.append(self.query(facet.facet_query, top_k=3, use_graph=False))
        
        search_results = await asyncio.gather(*search_tasks)
        
        # 3. ê²°ê³¼ í†µí•© (ë‹¨ìˆœ í•©ì§‘í•© + ë¦¬ë­í‚¹ ìœ ë„)
        unique_nodes = {}
        for res in search_results:
            # RetrieverQueryEngineì˜ ê²°ê³¼ëŠ” Response ê°ì²´ì´ë©°, source_nodesì— ë…¸ë“œë“¤ì´ ìˆìŒ
            if hasattr(res, 'source_nodes'):
                for node_with_score in res.source_nodes:
                    node_id = node_with_score.node.node_id
                    if node_id not in unique_nodes:
                        unique_nodes[node_id] = node_with_score
        
        final_nodes = list(unique_nodes.values())
        
        # 4. ìµœì¢… ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ë° ì‚¬ìœ  ìƒì„±
        context_docs = "\n\n".join([
            f"[ì¶”ì²œ ê¸°ì‚¬: {n.node.metadata.get('title')}]\në‚´ìš© ìš”ì•½: {n.node.metadata.get('summary', 'ìš”ì•½ ì—†ìŒ')}"
            for n in final_nodes[:top_k]
        ])
        
        recommendation_prompt = (
            f"ì‚¬ìš©ìê°€ ì‘ì„±í•œ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {analysis_result.core_summary}\n\n"
            f"ë‹¤ìŒì€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤:\n{context_docs}\n\n"
            f"ìœ„ ê¸°ì‚¬ë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ìì˜ ê¸°ì‚¬ì™€ ê°€ì¥ ìœ ì‚¬í•˜ê±°ë‚˜ ìƒí˜¸ ë³´ì™„ì ì¸ ë‰´ìŠ¤ 3ê°œë¥¼ ì„ ì •í•˜ê³ , "
            f"ê°ê° ì™œ ì¶”ì²œí•˜ëŠ”ì§€ 'ì˜ë¯¸ì  ìœ ì‚¬ì„±' ê´€ì ì—ì„œ ì„¤ëª…í•´ì¤˜."
        )
        
        final_response = self.llm.complete(recommendation_prompt)
        return final_response
