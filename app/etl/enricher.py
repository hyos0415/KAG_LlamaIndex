import os
import json
import asyncio
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Document, Settings, VectorStoreIndex
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.core.program import LLMTextCompletionProgram

from app.etl.storage import StorageManager
import nest_asyncio
nest_asyncio.apply()

class NewsMetadata(BaseModel):
    category: str = Field(description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì£¼ìš” ì¹´í…Œê³ ë¦¬ (ê²½ì œ, IT, ì‚¬íšŒ, ì •ì¹˜ ë“±)")
    sentiment: str = Field(description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ ê°ì„± (ê¸ì •, ì¤‘ë¦½, ë¶€ì •)")
    keywords: List[str] = Field(description="ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ")
    summary: str = Field(description="ê¸°ì‚¬ì˜ ë‚´ìš©ì„ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½")

class NewsEnricher:
    def __init__(self, model_name: str = "claude-sonnet-4-0"):
        """
        ë‰´ìŠ¤ ê³ ë„í™” ë° ì ìž¬ ì—”ì§„ ì´ˆê¸°í™”
        """
        self.model_name = model_name
        
        # LLM ë° ìž„ë² ë”© ì„¤ì •
        self.llm = Anthropic(model=self.model_name, timeout=300.0)
        self.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        
        # LlamaIndex ì „ì—­ ì„¤ì • ì ìš©
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        
        # StorageManager ì´ˆê¸°í™”
        self.storage_manager = StorageManager()
        
        # Semantic Splitter ì„¤ì •
        self.node_parser = SemanticSplitterNodeParser(
            buffer_size=1, breakpoint_percentile_threshold=95, embed_model=self.embed_model
        )

    async def _extract_metadata(self, content: str) -> NewsMetadata:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        prompt_template_str = (
            "ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\n"
            "ê¸°ì‚¬ ë‚´ìš©: {content}\n"
        )
        program = LLMTextCompletionProgram.from_defaults(
            output_cls=NewsMetadata,
            prompt_template_str=prompt_template_str,
            llm=self.llm,
            verbose=True
        )
        return program(content=content)

    async def process_and_load(self, file_path: str, limit: Optional[int] = None):
        """
        JSON ë‰´ìŠ¤ íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  T (Transform) & L (Load) ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        
        if limit:
            news_data = news_data[:limit]
            
        documents = []
        for item in news_data:
            content = item.get('content')
            if content:
                print(f"ðŸ” '{item.get('title')}' ê¸°ì‚¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                metadata = await self._extract_metadata(content)
                
                # 1. RDBMS ì €ìž¥
                article_record = {
                    "news_id": item.get('id', 'N/A'),
                    "title": item.get('title', 'N/A'),
                    "content": content,
                    "url": item.get('url', 'N/A'),
                    "pub_date": item.get('pub_date', 'N/A'),
                    "category": metadata.category,
                    "sentiment": metadata.sentiment,
                    "summary": metadata.summary,
                    "keywords": metadata.keywords
                }
                self.storage_manager.save_article_metadata(article_record)
                
                # 2. Document ìƒì„±
                doc = Document(
                    text=content,
                    metadata={
                        "title": article_record["title"],
                        "url": article_record["url"],
                        "pub_date": article_record["pub_date"],
                        "news_id": article_record["news_id"],
                        "category": metadata.category,
                        "sentiment": metadata.sentiment,
                        "keywords": ", ".join(metadata.keywords),
                        "summary": metadata.summary
                    }
                )
                documents.append(doc)
        
        # 3. Vector DB ì ìž¬ (Elasticsearch)
        if documents:
            print(f"ðŸš€ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ Elasticsearchì— ì ìž¬í•©ë‹ˆë‹¤...")
            nodes = self.node_parser.get_nodes_from_documents(documents)
            
            # Elasticsearchì— ì ìž¬
            es_storage_context = self.storage_manager.get_storage_context(store_type="elasticsearch")
            VectorStoreIndex(nodes, storage_context=es_storage_context)
            
            # (ì˜µì…˜) ê¸°ì¡´ ì‚¬ìš©ìžë¥¼ ìœ„í•´ ChromaDBì—ë„ ë³‘í–‰ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ëž˜ ì£¼ì„ í•´ì œ ê°€ëŠ¥
            # chroma_storage_context = self.storage_manager.get_storage_context(store_type="chroma")
            # VectorStoreIndex(nodes, storage_context=chroma_storage_context)
            
            print("âœ… Elasticsearch ì ìž¬ ì™„ë£Œ.")
        
        return documents

async def run_etl_pipeline(data_file: Optional[str] = None):
    """
    Airflow ìž‘ì—… ë“±ìœ¼ë¡œ ì‹¤í–‰ë  ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤
    """
    from dotenv import load_dotenv
    load_dotenv()
    
    if not data_file:
        base_dir = "/opt/airflow/result/airflow"
        if os.path.exists(base_dir):
            files = [os.path.join(base_dir, f) for f in os.listdir(base_dir) if f.endswith(".json")]
            if files:
                data_file = max(files, key=os.path.getmtime)
                print(f"ðŸ“‚ ìµœì‹  ë°ì´í„° íŒŒì¼ì„ ìžë™ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {data_file}")
    
    if not data_file or not os.path.exists(data_file):
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    enricher = NewsEnricher()
    try:
        await enricher.process_and_load(data_file)
        print("âœ… ETL íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise e

if __name__ == "__main__":
    import sys
    target_file = sys.argv[1] if len(sys.argv) > 1 else None
    asyncio.run(run_etl_pipeline(target_file))
