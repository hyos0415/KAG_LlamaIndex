import os
import json
import asyncio
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Document, PropertyGraphIndex, Settings, StorageContext, load_index_from_storage
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.core.program import LLMTextCompletionProgram

from app.builder.storage_manager import StorageManager
import nest_asyncio
nest_asyncio.apply()

class NewsMetadata(BaseModel):
    category: str = Field(description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì£¼ìš” ì¹´í…Œê³ ë¦¬ (ê²½ì œ, IT, ì‚¬íšŒ, ì •ì¹˜ ë“±)")
    sentiment: str = Field(description="ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ ê°ì„± (ê¸ì •, ì¤‘ë¦½, ë¶€ì •)")
    keywords: List[str] = Field(description="ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ")
    summary: str = Field(description="ê¸°ì‚¬ì˜ ë‚´ìš©ì„ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½")

class ClaudeNewsBuilder:
    def __init__(self, model_name: str = "claude-sonnet-4-0", storage_dir: str = "/opt/airflow/storage_claude"):
        """
        Claude ê¸°ë°˜ ë‰´ìŠ¤ ì§€ëŠ¥í˜• ë¹Œë” ì´ˆê¸°í™”
        """
        self.model_name = model_name
        self.storage_dir = storage_dir
        
        # LLM ë° ìž„ë² ë”© ì„¤ì •
        self.llm = Anthropic(model=self.model_name, timeout=300.0)
        self.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        
        # LlamaIndex ì „ì—­ ì„¤ì • ì ìš©
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        
        # StorageManager ì´ˆê¸°í™”
        self.storage_manager = StorageManager()
        
        # Semantic Splitter ì„¤ì •
        self.node_parser = SemanticSplitterNodeParser(
            buffer_size=1, breakpoint_percentile_threshold=95, embed_model=self.embed_model
        )
        
        self.index = None

    async def _extract_metadata(self, content: str) -> NewsMetadata:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        prompt_template_str = (
            "ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\n"
            "ê¸°ì‚¬ ë‚´ìš©: {content}\n"
        )
        program = LLMTextCompletionProgram.from_defaults(
            output_cls=NewsMetadata,
            prompt_template_str=prompt_template_str,
            llm=self.llm,
            verbose=True
        )
        return program(content=content)

    async def load_news_documents(self, file_path: str, limit: Optional[int] = None) -> List[Document]:
        """
        JSON ë‰´ìŠ¤ íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  T (Transform) ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        
        if limit:
            news_data = news_data[:limit]
            
        documents = []
        for item in news_data:
            content = item.get('content')
            if content:
                # 1. T (Transform): ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                print(f"ðŸ” '{item.get('title')}' ê¸°ì‚¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                metadata = await self._extract_metadata(content)
                
                # 2. L (Load): RDBMS ì €ìž¥
                article_record = {
                    "news_id": item.get('id', 'N/A'),
                    "title": item.get('title', 'N/A'),
                    "content": content,
                    "url": item.get('url', 'N/A'),
                    "pub_date": item.get('pub_date', 'N/A'),
                    "category": metadata.category,
                    "sentiment": metadata.sentiment,
                    "summary": metadata.summary,
                    "keywords": metadata.keywords
                }
                self.storage_manager.save_article_metadata(article_record)
                
                # 3. LlamaIndex Document ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
                doc = Document(
                    text=content,
                    metadata={
                        "title": article_record["title"],
                        "url": article_record["url"],
                        "pub_date": article_record["pub_date"],
                        "news_id": article_record["news_id"],
                        "category": metadata.category,
                        "sentiment": metadata.sentiment,
                        "keywords": ", ".join(metadata.keywords),
                        "summary": metadata.summary
                    }
                )
                documents.append(doc)
        
        print(f"âœ… {len(documents)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œ ë° ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
        return documents

    def build_graph(self, documents: List[Document], persist: bool = True):
        """
        ë‰´ìŠ¤ ë¬¸ì„œë“¤ì„ ì§€ì‹ ê·¸ëž˜í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (L: Vector DB ë° Graph ì €ìž¥)
        """
        print(f"ðŸš€ '{self.model_name}' ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ì§€ì‹ ì¶”ì¶œ ë° ê·¸ëž˜í”„ êµ¬ì¶•ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")
        
        # ì‹œë§¨í‹± ì²­í‚¹ ì ìš©
        nodes = self.node_parser.get_nodes_from_documents(documents)
        
        # ì¶”ì¶œê¸° ì„¤ì •
        extractor = SimpleLLMPathExtractor(
            llm=self.llm,
            num_workers=2
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì €ìž¥ì†Œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        storage_context = self.storage_manager.get_storage_context()
        
        # ê¸°ì¡´ ì €ìž¥ì†Œ ì‚­ì œ (ìƒˆë¡œ êµ¬ì¶• ì‹œ)
        if persist and os.path.exists(self.storage_dir):
            import shutil
            shutil.rmtree(self.storage_dir)
        
        self.index = PropertyGraphIndex(
            nodes=nodes,
            kg_extractors=[extractor],
            storage_context=storage_context,
            show_progress=True
        )
        
        if persist:
            # PropertyGraphIndex ìžì²´ ë©”íƒ€ë°ì´í„° ì €ìž¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬
            os.makedirs(self.storage_dir, exist_ok=True)
            self.index.storage_context.persist(persist_dir=self.storage_dir)
            print(f"ðŸ’¾ ì§€ì‹ ê·¸ëž˜í”„ ë©”íƒ€ë°ì´í„°ê°€ '{self.storage_dir}'ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        return self.index

    def query(self, query_str: str):
        """
        êµ¬ì¶•ëœ ê·¸ëž˜í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if not self.index:
            if os.path.exists(self.storage_dir):
                storage_context = self.storage_manager.get_storage_context()
                # ì €ìž¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ (ChromaDBì™€ ì—°ê²°ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
                # PropertyGraphIndex.from_storage ê°€ í˜„ìž¬ ë²„ì „ì—ì„œ ê³µì‹ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸ í•„ìš”
                # ì¼ë°˜ì ì¸ ê²½ìš° load_index_from_storage ë“±ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì¸ì¶œ ë°©ì‹ í™•ì¸
                try:
                    self.index = PropertyGraphIndex.from_storage(storage_context)
                except:
                    # ëŒ€ì²´ ë¡œë“œ ë°©ì‹ (ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ)
                    self.index = load_index_from_storage(storage_context)
            else:
                raise ValueError("êµ¬ì¶•ëœ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. build_graphë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                
        query_engine = self.index.as_query_engine(include_text=True)
        return query_engine.query(query_str)

async def run_pgi_pipeline(data_file: Optional[str] = None):
    """
    Airflow ìž‘ì—… ë“±ìœ¼ë¡œ ì‹¤í–‰ë  ì „ì²´ PGI íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤
    """
    from dotenv import load_dotenv
    load_dotenv()
    
    # ë°ì´í„° íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìµœì‹  íŒŒì¼ ì°¾ê¸°
    if not data_file:
        base_dir = "/opt/airflow/result/airflow"
        if os.path.exists(base_dir):
            files = [os.path.join(base_dir, f) for f in os.listdir(base_dir) if f.endswith(".json")]
            if files:
                data_file = max(files, key=os.path.getmtime)
                print(f"ðŸ“‚ ìµœì‹  ë°ì´í„° íŒŒì¼ì„ ìžë™ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {data_file}")
    
    if not data_file or not os.path.exists(data_file):
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    builder = ClaudeNewsBuilder()
    
    try:
        # ë°ì´í„° ë¡œë“œ ë° ê³ ë„í™” (T & L: RDBMS)
        docs = await builder.load_news_documents(data_file)
        
        # ì§€ì‹ ê·¸ëž˜í”„ êµ¬ì¶• (L: Vector DB & Graph)
        builder.build_graph(docs)
        
        print("âœ… PGI íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        # ì—ì–´í”Œë¡œìš°ì—ì„œ ì‹¤íŒ¨ë¡œ ì¸ì‹í•˜ë„ë¡ ì˜ˆì™¸ ìž¬ë°œìƒ
        raise e

if __name__ == "__main__":
    import sys
    # ì¸ìžë¡œ íŒŒì¼ ê²½ë¡œë¥¼ ë„˜ê²¨ë°›ì„ ìˆ˜ ìžˆë„ë¡ í•¨
    target_file = sys.argv[1] if len(sys.argv) > 1 else None
    asyncio.run(run_pgi_pipeline(target_file))
